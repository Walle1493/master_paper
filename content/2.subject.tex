\chapter{任务定义及评价方法}
% 机器阅读理解是什么
机器阅读理解是指让计算机能够像人类一样理解和回答自然语言文本中的问题，它是自然语言处理技术中的一个重要分支。
机器阅读理解是指让机器模拟人类阅读理解的能力，即让机器能够自动地从文本中抽取出问题的答案。
它是自然语言处理中的一个重要任务。
几年来，将预训练语言模型应用于机器阅读理解任务中，已经取得了很好的效果。

% 长文本机器阅读理解是什么
长文本机器阅读理解是在机器阅读理解的基础上进一步发展，以解决输入长文本篇幅的问题。
这项技术在处理大量长文本时具有重要价值，能够帮助机器有效提取和分析文本中的信息和关系，从而节省人工阅读和分析的时间和成本。
随着数字化时代和互联网的兴起，人们面临的信息量越来越大，长文本机器阅读理解技术可以帮助人类更好地理解和利用这些信息。
此外，该技术还可以整合更多信息和背景知识，以提高智能问答和文本自动生成等应用的准确性和效率。
然而，长文本机器阅读理解也面临着挑战，例如长文本中存在大量的自然语言表达的多样性和歧义，需要机器具备处理这些多义性和歧义性的能力，以正确理解文本的含义。
此外，长文本还涉及到大量上下文信息和推理过程，需要机器具备对上下文信息的理解和推理能力。

% 本文是如何做研究的
本文旨在解决长文本阅读理解领域的共性问题和各个细分领域的难点，基于不同领域的长文本数
据集，提出了三种不同的方法。
本章将详细介绍面向长文本的机器阅读理解的任务定义，实验过程中所采用的的语料，以及该任务的相关评价指标。

\section{任务定义}
长文本机器阅读理解的任务定义等同于常规的机器阅读理解，可以表述为：
给定一个篇幅较长的文本文档$D$（Document）和一个问题$Q$（Question），目标是输出与该问题$Q$相关的文本片段或答案$A$（Answer）。
假设文本文档$D$表示为一个由$n$个单词组成的序列$D = [w_1, w_2, ..., w_n]$。
%通常，长文本阅读理解模型需要将该长文本截取为一个由m个单词组成的短文本序列$C = [u_1, u_2, ..., u_m]$。
问题$Q$表示为一个由$m$个单词组成的序列$Q = [q_1, q_2, ..., q_m]$，那么机器阅读理解的任务可以数学化的表示为：
% $\hat{a} = \operatorname*{argmax}_{a \in A} \operatorname{sim}(f_D(D), f_Q(Q, a))$

$$A = f(D, Q)$$

公式中的$f$是一个需要让机器去学习的预测函数，通过向函数$f$提供文本文档$D$和问题$Q$，预测出答案$A$。
根据答案风格和表现方式的不同，长文本机器阅读理解任务可以归纳为以下三种类型\footnote{实际上，机器阅读理解任务按照答案类型可以分为抽取式、生成式、完形填空和多项选择四种，本文的实验数据采用了NewsQA，MuSiQue和QuALITY，分别为抽取式和多项选择类型的答案，并涉及了生成式的方法。因此，本文主要考虑这三种类型的机器阅读理解任务。}：

\input{table/2-1.tex}

（1）抽取式（Extractive）

抽取式阅读理解任务是指给定一篇文本和一个问题，模型需要从给定的文本段落中“抽取”出正确的答案片段来回答问题。
抽取式阅读理解实际上是一个分类问题，它要求机器预测答案的开始位置$pos_{start}$与结束位置$pos_{end}$。
最终答案可以表示为$D[pos_{start}:pos_{end}+1]$。
如表~\ref{tab:2-1}~中的抽取式样例所示，列举了NewsQA数据集中的一个样例。
对于问题“When was Pandher sentenced to death?”，答案“February”对应文档中的一个连续的自然文本片段。

（2）生成式（Generative）

生成式阅读理解要求机器基于给定的文本和问题，生成一个合适的答案，该答案不局限于文章中存在的词语，而是自由生成的。
这种任务更适合实际生活场景，但是由于生成的句子无法做准确评估，因此一直无法成为业界的主流数据集。
如表~\ref{tab:2-1}~中的生成式样例所示，列举了NarrativeQA数据集中的一条数据。
对于问题“Why does Dorothy want to go home?”，答案“Because she misses her family and friends.”可以不是文档中的一个连续的文本片段，而是一个自由阐述的文本观点。

（3）多项选择（Multiple Choice）

多项选择阅读理解要求机器基于给定的文章和问题，从多个备选答案中选择一个最有可能是正确答案的选项。
这种任务类似于英语考试中的阅读理解选择题。
当前处理这类问题的模型基本都采用<问题，文章，答案>三元组的框架。
如表~\ref{tab:2-1}~中的多项选择样例所示，列举了QuALITY数据集中的一个样例。
对于问题“Why did Roylott want to kill Helen?”和四个候选答案，机器需要从这四个答案中选出最合适的一个。


\section{语料资源}
本文在新闻长文本阅读理解数据集NewsQA，多跳多文档阅读理解数据集MuSiQue，以及多项选择长文本阅读理解数据集QuALITY三个公开语料上进行相关实验。
本节将分别介绍这三个数据集的详细信息。

\subsection{新闻长文本数据集NewsQA}
% 概述
NewsQA是由纽约大学、卡耐基梅隆大学和麻省理工学院联合推出的一个大规模新闻阅读理解数据集。该数据集覆盖了新闻报道中的多个主题和事件，涉及政治、经济、文化等各个领域，旨在为机器阅读理解任务提供具有挑战性的现实世界应用场景。

% 样本分布（长度、数量等）
（1）样本分布

NewsQA数据集包括超过10,000篇新闻文章，以及超过100,000个与这些文章相关的问答对。
其训练/开发/测试集的分布如~\ref{tab:2-2}~所示。
此外，表格中还统计了与文本长度相关的数据指标。
表格中的TPP表示每篇文章中的token数量（tokens per passage），PPP表示每篇文章中的段落数量（paragraphs per passage）。
从这些数据指标可以看出，大多数文本长度超出了512这个长度限制。
实际上，NewsQA中的文本还包含了复杂的语句和语法结构。
在后续方法中，本文也着重考虑到处理大量数据和复杂数据结构的问题。
另外，至少有一半的文本，其段落数量达到了18。
这对于段落检索也造成了一定的挑战。

\input{table/2-2.tex}

% 文本风格（问题、文本类型等）
（2）文本风格

NewsQA数据集的特点在于其问题和答案是由人工构造的，并且问题涉及多种类型，如实体识别、原因分析、情感分析、时间和日期等多个方面，从而涵盖了各种常见问题类型。
其次，NewsQA中的问题往往需要依赖于上下文来进行回答，而不是简单地基于问题本身。
这意味着处理NewsQA数据需要考虑到上下文的语义信息。
另外，NewsQA中的文本是由不同的新闻文章组成的，这些文章的句子结构和语法风格可能会有所不同。
因此，在处理 NewsQA 数据时需要考虑到句子结构和语法的多样性。
最后，NewsQA数据集中的问题还包含了人类提问时可能存在的模糊性、歧义性和主观性等因素，因此对机器阅读理解模型的能力提出了更高的要求。

% 小结
NewsQA数据集已成为机器阅读理解任务中的重要标准基准数据集之一，被广泛应用于学术界和工业界的相关研究工作中。

\subsection{多跳数据集MuSiQue}
% 概述
MuSiQue是一个多跳阅读理解数据集，它是专门为多跳阅读理解任务而设计的。
该数据集通过其他机器阅读理解数据集，使用自下而上的方法进行构建，该方法系统地选择相互关联的可组合成对单跳问题，其中一个推理步骤主要依赖于来自另一个步骤的信息。
这种方法使作者能够探索广阔的问题空间，并添加严格的过滤器以及其他针对关联推理的机制。
它可以对构造过程和由此产生的k-hop问题的属性进行精细控制。
每个问题都需要回答一个自然语言问题，其中问题的答案可能需要跨越多个句子。
为了回答这些问题，需要进行多次推理和跳转，涉及多个句子和段落。


% 样本分布（长度、数量等）
（1）样本分布

MuSiQue数据集总共包含超过4,000个多句子文档，以及超过10,000个与这些文档相关的问答对。
训练/开发集的分布如~\ref{tab:2-3}~所示。
表格中的后两列是与文本长度相关的统计数据。
其中，对于超过99\%以上的问题来说，提供的文档数为20个；
另外不到1\%的问题提供少于20个文档。
TPD表示每个MuSiQue文档中的平均token数量（tokens per document）。
从这些数据指标可以看出，针对每个提问，需要阅读的token数量是非常多的，平均可以达到2,000以上，这意味着相比于其他的阅读理解数据结构，MuSiQue涵盖了更多的语义信息和上下文信息。

\input{table/2-3.tex}

更重要的是，MuSiQue包含了大量2-4跳的问题，如表~\ref{tab:2-4}~所示，展示了一些典型的问题。
在后续方法中，本文着重考虑如何将多跳问题分解为一些简单的单跳问题，从而进行下一步工作。

\input{table/2-4.tex}

% 文本风格（问题、文本类型等）
（2）文本风格

MuSiQue包含2-4跳的问题，每个问题都需要对多个句子进行理解和推断。
与其他机器阅读理解数据集相比，MuSiQue数据集中的问题具有较高的复杂度和挑战性，因为它需要对故事中的多个句子进行理解、推理和跳转。
此外，该数据集还包含各种类型的问题，包括原因、解释、转折和逻辑推理等。
MuSiQue数据集的发布可以促进研究人员在多跳阅读理解任务上进行深入研究和评估。

\subsection{多项选择数据集QuALITY}
% 概述
QuALITY数据集是由纽约大学的研究者创建的。
研究者采集了来自于维基百科、英文小说和新闻文章，来构成阅读文本。
这些文本涵盖了多种主题，如历史、科学、文学、政治等。
QuALITY由人工编写问题，每个问题有四个选项，其中一个是正确答案。

QuALITY数据集的难点和挑战在于段落的长度和复杂性，以及问题的多样性和深度。
这个数据集有助于提高长文本理解的能力，对于一些需要处理长篇文章或书籍的应用场景很有价值。

% 样本分布（长度、数量等）
（1）样本分布

QuALITY数据集收录了381个左右的长篇文档，这些文章的平均长度在4,700个英文单词左右。
具体的训练/开发/测试集的数据分布如~\ref{tab:2-5}~所示。
同时，QuALITY中包含了一部分困难问题，这些困难问题由数据标注者标出他们认为难以回答的问题。

\input{table/2-5.tex}

% 文本风格（问题、文本类型等）
（2）文本风格

相比抽取式阅读理解数据集，多项选择数据集QuALITY更注重逻辑推理能力。
数据集中提问的风格各不相同，有些是事实性的询问，有些是推理性的判断，有些是意见性的评价。
表~\ref{tab:2-6}~中随机抽取了500条数据，统计其推理类型。

\input{table/2-6.tex}

\section{性能评价指标}
对于抽取式阅读理解数据集NewsQA和MuSiQue，本文采用精准匹配EM和调和匹配F1来衡量模型预测答案文本片段的准确性；
同时，针对MuSiQue中需要生成问题的阶段，本文也采用了BLEU，METEOR和ROUGE等指标来评估生成的可靠程度。
对于多项选择数据集QuALITY，本文采用准确率Acc来评估模型在多个备选答案中选择正确答案的能力。

针对抽取式阅读理解数据集NewsQA和MuSiQue，本文选择了EM和F1这两个指标来衡量模型预测答案文本片段的准确性。
此外，针对MuSiQue中需要生成问题的阶段，本文还使用了BLEU、METEOR和ROUGE等指标来评估生成的可靠程度。
对于多项选择数据集QuALITY，本文采用了准确率Acc来评估模型在多个备选答案中选择正确答案的能力。
以上指标的选择充分考虑了不同任务的特点，以确保模型的性能能够得到全面的评估。

% \subsection{精准匹配EM}
（1）EM

EM（Exact Match）指标用于衡量模型的预测答案是否与真实答案完全匹配。
当模型的预测答案与真实答案完全一致时，EM值为1；反之，如果预测答案与真实答案不完全匹配，EM值为0。
EM指标的优点是简单直观，能够快速评估模型的准确性，但缺点是对于稍有不同的答案就会评估为错误，对模型的容错性要求较高。

% \subsection{调和匹配F1}
（2）F1

F1是一种综合考虑模型精精确率（Precision，P）和召回率（Recall，R）的指标，通常用于评估二分类或多分类任务中的模型性能。
在抽取式阅读理解任务中，F1被用来衡量模型预测答案文本片段的准确性。
它是精确率和召回率的加权平均值，用公式表述为：

$$F1=\frac{2*P*R}{P+R}$$

其中，$P$是指模型正确预测的token数量与总预测token数量的比率，即TP/(TP+FP)；
$R$是指模型正确预测的token数量与真实token数量的比率，即TP/(TP+FN)。
其中，TP表示真正例，即模型正确预测为正例的token数量；
FP表示假正例，即模型错误地将负例预测为正例的token数量；
FN表示假负例，即模型错误地将正例预测为负例的token数量。
F1的取值范围为0到1，越接近1表示模型的性能越好。
并且F1更注重模型的综合表现

% \subsection{生成指标BLEU和METEOR}
% （3）相关生成指标

（3）BLEU
BLEU（Bilingual Evaluation Understudy）是一种用来评估机器翻译质量以及其他生成任务的指标，它通过计算候选文本和参考文本之间的n元语法匹配度来衡量它们的相似度。

% BLEU指标的计算方法如下：
% 对于每个生成的句子，计算它与参考句子之间的n-gram重叠数量；
% 对于每个n-gram，计算它在生成句子中出现的最大次数和在参考句子中出现的次数之间的差值；
% 计算BLEU分数，即将n-gram重叠数量的几何平均值除以生成句子中的总词数，并对结果进行惩罚以避免过度依赖短句子。

BLEU的计算公式可以表述如下：

$$BLEU = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)$$

其中，BP是惩罚因子，用来防止过短的候选文本获得高分。
它定义为：

$$
BP = 
\begin{cases}
1 & \text{if } c > r \\
e^{(1-r/c)} & \text{if } c \leq r
\end{cases}
$$

公式中，c是候选文本的长度，r是参考文本的有效长度（即与c最接近的长度）；
N是最大的n元语法长度，通常取4；
wn是权重系数，通常取1/N；
pn是n元语法精确度，即候选文本中与参考文本匹配的n元语法数量除以候选文本中所有n元语法数量。

BLEU指标的取值范围为0到1，值越高表示机器翻译的结果与参考结果越接近。

（4）METEOR

METEOR（Metric for Evaluation of Translation with Explicit ORdering）是一种基于单词精确度和召回率的调和平均数的指标，同时考虑了词干、同义词和词序的匹配，常用于评估机器翻译系统的生成结果。

% METEOR指标的计算方法如下：
% 对于生成句子和参考句子，分别计算它们的词汇语义相似度得分。
% 计算生成句子与参考句子之间的词汇重叠数量和不重叠词的数量。
% 根据相似度得分、重叠数量和不重叠词的数量计算METEOR分数。
% METEOR指标的取值范围为0到1，值越高表示机器翻译的结果与参考结果越接近。

METEOR的计算公式如下：

$$METEOR = \frac{10 P R}{R + 9 P} (1 - Penalty)$$

其中，P是单词精确度，即候选文本中与参考文本匹配的单词数量除以候选文本中所有单词数量；
R是单词召回率，即候选文本中与参考文本匹配的单词数量除以参考文本中所有单词数量；
Penalty是惩罚因子，用来降低过多切分或乱序的候选文本得分。
它定义为：

$$Penalty = 0.5 (\frac{Chunks}{Matches})^3$$

公式中的Chunks是候选文本中与参考文本匹配的连续单词块数量；
Matches是候选文本中与参考文本匹配的总单词数量。

METEOR的取值范围是0到1之间

% 总的来说，BLEU指标更加注重短语的匹配程度，适用于评估机器翻译、自动摘要等任务。而METEOR指标则更加注重句子级别的语义相似性，适用于评估机器翻译、自动文本评价等任务。

（5）ROUGE

ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一种用于评估自然语言生成系统输出的指标，它是以召回率为导向的评估指标，旨在衡量系统生成的文本与参考答案之间的重叠程度。

ROUGE指标分为多个变种，本文用到的是ROUGE-N，它测量系统生成文本与参考答案中N个连续词汇的重叠程度。
其计算公式为：

$$ROUGE_{N} = \frac{\sum\limits_{r\in\text{Reference}}\sum\limits_{n\in\text{n-grams}}Count_n(r)\cdot \min{Count_n(r), Count_n(\text{System})}}{\sum\limits_{r\in\text{Reference}}\sum\limits_{n\in\text{n-grams}}Count_n(r)}$$

其中，$n$表示$n$-gram的长度，$r$表示参考答案，$\text{System}$表示自动生成的文本。$Count_n(r)$表示在参考答案中$n$-gram $n$在$r$中出现的次数，$\min{Count_n(r), Count_n(\text{System})}$表示参考答案和自动生成的文本中$n$-gram $n$的最小出现次数。$\sum\limits_{n\in\text{n-grams}}$表示对所有长度为$n$的$n$-gram求和，$\sum\limits_{r\in\text{Reference}}$表示对所有参考答案求和。


% \subsection{准确率Acc}
（6）ACC

在多项选择阅读理解任务中，ACC（Accuracy）是最常用的评价指标之一，它衡量模型对问题的正确率。

% ACC的计算方式是将模型的预测答案与正确答案进行比较，如果二者一致，则计数器加1，最后将正确预测的问题数量除以问题总数得到准确率。
准确率是最常见的分类评价指标，其计算公式为模型分类正确的样本数量除以总样本数量。
其计算公式如下：

$$ACC=\frac{TP+TN}{P+N}$$

其中，P表示正例样本数量，N表示负例样本数量，TP和TN分别表示模型预测正确的正负例样本数量。

\section{本章小结}
本章首先对面向长文本的机器阅读理解任务进行了明确定义；
接着，本章从三个数据集的样本分布、语言风格等多方面对数据集进行了分析，指出了不同数据集的特点和难点，为后续的研究和模型选择提供了参考；
最后，本文介绍了几个关键的评价指标，这些指标可以用于评估模型的性能和指导模型的优化和改进。
